{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_7B = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "LLAMA_3_1_8B = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "LLAMA_3_1_70B = \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8\"\n",
    "\n",
    "LLAMA_3_8B = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "LLAMA_3_70B = \"neuralmagic/Meta-Llama-3-70B-Instruct-FP8\"\n",
    "\n",
    "\n",
    "LLAMA_2_70B = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "LLAMA_2_13B = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "LLAMA_2_7B = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "QWEN_2_7B = \"Qwen/Qwen2-7B-Instruct\"\n",
    "QWEN_2_72B = \"Qwen/Qwen2-72B-Instruct-GPTQ-Int8\"\n",
    "\n",
    "PHI_3_MEDIUM = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "PHI_3_5_MINI = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "PHI_3_5_MOE = \"microsoft/Phi-3.5-MoE-instruct\"\n",
    "\n",
    "GEMMA_2_9B = \"google/gemma-2-9b-it\"\n",
    "GEMMA_2_2B = \"google/gemma-2b-it\"\n",
    "GEMMA_2_27B = \"google/gemma-2-27b-it\"\n",
    "GPT_4_OMNI_MINI = \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "GPT_4 = \"gpt-4-0613\"\n",
    "GPT_3 = \"text-davinci-003\"\n",
    "\n",
    "MODELS = [\n",
    "    MISTRAL_7B,\n",
    "    LLAMA_3_1_8B,\n",
    "    LLAMA_3_1_70B,\n",
    "    LLAMA_3_8B,\n",
    "    LLAMA_3_70B,\n",
    "    LLAMA_2_70B,\n",
    "    LLAMA_2_13B,\n",
    "    LLAMA_2_7B,\n",
    "    QWEN_2_7B,\n",
    "    QWEN_2_72B,\n",
    "    PHI_3_MEDIUM,\n",
    "    PHI_3_5_MINI,\n",
    "    PHI_3_5_MOE,\n",
    "    GEMMA_2_9B,\n",
    "    GEMMA_2_2B,\n",
    "    GEMMA_2_27B,\n",
    "    GPT_4_OMNI_MINI,\n",
    "    GPT_4,\n",
    "    GPT_3,\n",
    "]\n",
    "\n",
    "\n",
    "# LLM models information dictionary\n",
    "llm_models = {\n",
    "    MISTRAL_7B: {\n",
    "        \"release_date\": \"2023-12\",\n",
    "        \"pretty_name\": \"Mistral 7B\",\n",
    "        \"family\": \"Mistral\",\n",
    "        \"size\": \"7B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    LLAMA_3_1_8B: {\n",
    "        \"release_date\": \"2024-07\",\n",
    "        \"pretty_name\": \"Llama 3.1 8B\",\n",
    "        \"family\": \"Llama\",\n",
    "        \"size\": \"8B\",\n",
    "        \"version\": \"3.1\",\n",
    "    },\n",
    "    LLAMA_3_1_70B: {\n",
    "        \"release_date\": \"2024-07\",\n",
    "        \"pretty_name\": \"Llama 3.1 70B\",\n",
    "        \"family\": \"Llama\",\n",
    "        \"size\": \"70B\",\n",
    "        \"version\": \"3.1\",\n",
    "    },\n",
    "    LLAMA_3_8B: {\n",
    "        \"release_date\": \"2024-04\",\n",
    "        \"pretty_name\": \"Llama 3 8B\",\n",
    "        \"family\": \"Llama\",\n",
    "        \"size\": \"8B\",\n",
    "        \"version\": \"3\",\n",
    "    },\n",
    "    LLAMA_3_70B: {\n",
    "        \"release_date\": \"2024-04\",\n",
    "        \"pretty_name\": \"Llama 3 70B\",\n",
    "        \"family\": \"Llama\",\n",
    "        \"size\": \"70B\",\n",
    "        \"version\": \"3\",\n",
    "    },\n",
    "    LLAMA_2_70B: {\n",
    "        \"release_date\": \"2023-07\",\n",
    "        \"pretty_name\": \"Llama 2 70B\",\n",
    "        \"family\": \"Llama\",\n",
    "        \"size\": \"70B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    LLAMA_2_13B: {\n",
    "        \"release_date\": \"2023-07\",\n",
    "        \"pretty_name\": \"Llama 2 13B\",\n",
    "        \"family\": \"Llama\",\n",
    "        \"size\": \"13B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    LLAMA_2_7B: {\n",
    "        \"release_date\": \"2023-07\",\n",
    "        \"pretty_name\": \"Llama 2 7B\",\n",
    "        \"family\": \"Llama\",\n",
    "        \"size\": \"7B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    QWEN_2_72B: {\n",
    "        \"release_date\": \"2024-06\",\n",
    "        \"pretty_name\": \"Qwen 2 72B\",\n",
    "        \"family\": \"Qwen\",\n",
    "        \"size\": \"72B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    QWEN_2_7B: {\n",
    "        \"release_date\": \"2024-06\",\n",
    "        \"pretty_name\": \"Qwen 2 7B\",\n",
    "        \"family\": \"Qwen\",\n",
    "        \"size\": \"7B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    PHI_3_MEDIUM: {\n",
    "        \"release_date\": \"2024-05\",\n",
    "        \"pretty_name\": \"Phi-3 Medium\",\n",
    "        \"family\": \"Phi\",\n",
    "        \"size\": \"14B\",\n",
    "        \"version\": \"3\",\n",
    "    },\n",
    "    PHI_3_5_MINI: {\n",
    "        \"release_date\": \"2024-08\",\n",
    "        \"pretty_name\": \"Phi-3.5 Mini\",\n",
    "        \"family\": \"Phi\",\n",
    "        \"size\": \"4B\",\n",
    "        \"version\": \"3.5\",\n",
    "    },\n",
    "    PHI_3_5_MOE: {\n",
    "        \"release_date\": \"2024-08\",\n",
    "        \"pretty_name\": \"Phi-3.5 MoE\",\n",
    "        \"family\": \"Phi\",\n",
    "        \"size\": \"42B\",\n",
    "        \"version\": \"3.5\",\n",
    "    },\n",
    "    GEMMA_2_9B: {\n",
    "        \"release_date\": \"2024-06\",\n",
    "        \"pretty_name\": \"Gemma 2 9B\",\n",
    "        \"family\": \"Gemma\",\n",
    "        \"size\": \"9B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    GEMMA_2_2B: {\n",
    "        \"release_date\": \"2024-07\",\n",
    "        \"pretty_name\": \"Gemma 2 2B\",\n",
    "        \"family\": \"Gemma\",\n",
    "        \"size\": \"2B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    GEMMA_2_27B: {\n",
    "        \"release_date\": \"2024-06\",\n",
    "        \"pretty_name\": \"Gemma 2 27B\",\n",
    "        \"family\": \"Gemma\",\n",
    "        \"size\": \"27B\",\n",
    "        \"version\": \"2\",\n",
    "    },\n",
    "    GPT_4_OMNI_MINI: {\n",
    "        \"release_date\": \"2024-07\",\n",
    "        \"pretty_name\": \"GPT-4o Mini\",\n",
    "        \"family\": \"GPT\",\n",
    "        \"size\": \"Mini\",\n",
    "        \"version\": \"4\",\n",
    "    },\n",
    "    GPT_4: {\n",
    "        \"release_date\": \"2024-06\",\n",
    "        \"pretty_name\": \"GPT-4\",\n",
    "        \"family\": \"GPT\",\n",
    "        \"size\": \"4\",\n",
    "        \"version\": \"4\",\n",
    "    },\n",
    "    GPT_3: {\n",
    "        \"release_date\": \"2022-11\",\n",
    "        \"pretty_name\": \"GPT-3\",\n",
    "        \"family\": \"GPT\",\n",
    "        \"size\": \"3\",\n",
    "        \"version\": \"3\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_model_release_date(model):\n",
    "    return llm_models[model][\"release_date\"]\n",
    "\n",
    "\n",
    "def get_family(model):\n",
    "    return llm_models[model][\"family\"]\n",
    "\n",
    "\n",
    "def get_size(model):\n",
    "    return llm_models[model][\"size\"]\n",
    "\n",
    "\n",
    "def get_version(model):\n",
    "    return llm_models[model][\"version\"]\n",
    "\n",
    "\n",
    "def get_pretty_name(model):\n",
    "    return llm_models[model][\"pretty_name\"]\n",
    "\n",
    "def country2alpha_2(country):\n",
    "    try:\n",
    "        country_code = self.LFM.country2alpha_2[country]\n",
    "    except:\n",
    "        import pycountry\n",
    "\n",
    "        try:\n",
    "            country_code = pycountry.countries.get(common_name=country).alpha_2.lower()\n",
    "        except:\n",
    "            try:\n",
    "                country_code = pycountry.countries.get(name=country).alpha_2.lower()\n",
    "            except:\n",
    "                import pdb\n",
    "\n",
    "                pdb.set_trace()\n",
    "    return country_code\n",
    "\n",
    "\n",
    "def language2alpha_2(language):\n",
    "    import pycountry\n",
    "\n",
    "    try:\n",
    "        alpha_2 = pycountry.languages.get(name=language).alpha_2.lower()\n",
    "        # [l.name for l in pycountry.languages]\n",
    "    except:\n",
    "        try:\n",
    "            alpha_2 = pycountry.languages.get(name=language).alpha_3.lower()\n",
    "        except:\n",
    "            import pdb\n",
    "\n",
    "            pdb.set_trace()\n",
    "    return alpha_2\n",
    "\n",
    "\n",
    "def get_suffix(add_paraphrase, country_code):\n",
    "    suffix = \"\"\n",
    "    if add_paraphrase:\n",
    "        suffix += \"_para\"\n",
    "    if country_code is not None:\n",
    "        suffix += f\"_{country_code}\"\n",
    "    return suffix\n",
    "\n",
    "\n",
    "def get_model_name_path(model_version):\n",
    "    return model_version.replace(\"/\", \"_\")\n",
    "\n",
    "\n",
    "import ast\n",
    "\n",
    "\n",
    "def convert_string_to_object(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "\n",
    "# import transformers\n",
    "\n",
    "\n",
    "class TransformerQuery:\n",
    "    _llm = None\n",
    "    _model_version = None\n",
    "\n",
    "    def __init__(self, model_version, max_tokens, system_prompt=None) -> None:\n",
    "        self.system_prompt = system_prompt\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        import torch\n",
    "        from vllm import LLM, SamplingParams\n",
    "\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "\n",
    "        if TransformerQuery._llm is None:\n",
    "            TransformerQuery._llm = LLM(\n",
    "                model_version,\n",
    "                max_num_seqs=4,\n",
    "                tensor_parallel_size=num_gpus,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "            TransformerQuery._model_version = model_version\n",
    "        assert model_version == TransformerQuery._model_version\n",
    "        self.llm = TransformerQuery._llm\n",
    "        self.sampling_params = SamplingParams(\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        self.model_version = model_version\n",
    "        # self.pipeline = transformers.pipeline(\n",
    "        #     \"text-generation\",\n",
    "        #     model=model_version,\n",
    "        #     device_map=\"auto\",\n",
    "        # )\n",
    "\n",
    "        # # Check that tokenizer has template\n",
    "        # if self.pipeline.tokenizer.chat_template is None:\n",
    "        #     raise ValueError(\"Tokenizer does not have chat template\")\n",
    "\n",
    "    def ask(self, query):\n",
    "        if self.system_prompt is None:\n",
    "            messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        else:\n",
    "            if \"gemma\" in self.model_version.lower():\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": self.system_prompt + \"\\n\" + query},\n",
    "                ]\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": query},\n",
    "                ]\n",
    "        # outputs = self.pipeline(\n",
    "        #     messages,\n",
    "        #     max_new_tokens=self.max_tokens,\n",
    "        #     do_sample=False,\n",
    "        #     top_k=None,\n",
    "        #     top_p=1.0,\n",
    "        #     temperature=None,\n",
    "        # )\n",
    "        outputs = self.llm.chat(\n",
    "            messages, sampling_params=self.sampling_params, use_tqdm=False\n",
    "        )\n",
    "\n",
    "        # return outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "        return outputs[0].outputs[0].text\n",
    "\n",
    "    def ask_batch(self, queries):\n",
    "        from vllm.entrypoints.llm import apply_chat_template, parse_chat_messages\n",
    "\n",
    "        prompts = []\n",
    "        for q in queries:\n",
    "            messages = []\n",
    "            if self.system_prompt is None:\n",
    "                messages = [{\"role\": \"user\", \"content\": q}]\n",
    "            else:\n",
    "                if \"gemma\" in self.model_version.lower():\n",
    "                    messages = [\n",
    "                        {\"role\": \"user\", \"content\": self.system_prompt + \"\\n\" + q},\n",
    "                    ]\n",
    "                else:\n",
    "                    messages = [\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": q},\n",
    "                    ]\n",
    "            tokenizer = self.llm.get_tokenizer()\n",
    "            model_config = self.llm.llm_engine.get_model_config()\n",
    "            conversations, _ = parse_chat_messages(messages, model_config, tokenizer)\n",
    "\n",
    "            prompt = apply_chat_template(\n",
    "                tokenizer,\n",
    "                conversations,\n",
    "                chat_template=None,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        outputs = self.llm.generate(\n",
    "            prompts, sampling_params=self.sampling_params, use_tqdm=True\n",
    "        )\n",
    "        return [o.outputs[0].text for o in outputs]\n",
    "\n",
    "\n",
    "def get_llm_model(model_version, max_tokens, system_prompt=None):\n",
    "    if \"gpt\" in model_version:\n",
    "        from efficiency.nlp import AzureChatbot, Chatbot\n",
    "\n",
    "        if \"z-\" in model_version:\n",
    "            return AzureChatbot(model_version, max_tokens, system_prompt=system_prompt)\n",
    "        return Chatbot(model_version, max_tokens, system_prompt=system_prompt)\n",
    "    else:\n",
    "        return TransformerQuery(model_version, max_tokens, system_prompt)\n",
    "\n",
    "\n",
    "# Translate MultiTP dataset\n",
    "dataset_file_tmpl = (\n",
    "    \"data/datasets/dataset_{lang}+{translator_provider_forward}{suffix}.csv\"\n",
    ")\n",
    "system_prompt_file_tmpl = (\n",
    "    \"data/datasets/system_prompt_{role}-{lang}+{translator_provider_forward}.csv\"\n",
    ")\n",
    "# Back translation of dataset (for analysis)\n",
    "dataset_back_translated_file_tmpl = \"data/datasets/dataset_{lang}+{translator_provider_forward}{suffix}_back_translated.csv\"\n",
    "\n",
    "# Query model\n",
    "cache_responses_tmpl = \"data/cache/{model_version}_{system_role}_{lang}+{translator_provider_forward}{suffix}_raw_resp.csv\"\n",
    "\n",
    "# Back translation\n",
    "cache_back_responses_tmpl = \"data/cache/{model_version}_{system_role}_{lang}+{translator_provider_forward}{suffix}_tr+{translator_provider_backward}_resp.csv\"\n",
    "\n",
    "# Parsing\n",
    "cache_parse_responses_tmpl = \"data/cache_parsing/B={analysis_backend_model_version}/{model_version}_{system_role}_{lang}+{translator_provider_forward}{suffix}_tr+{translator_provider_backward}_response.csv\"\n",
    "\n",
    "# Performance analsisi\n",
    "performance_file_tmpl = \"data/performance/B={analysis_backend_model_version}/{model_version}_{system_role}_{lang}+{translator_provider_forward}{suffix}_tr+{translator_provider_backward}_performance.csv\"\n",
    "performance_file_v2_tmpl = \"data/performance/B={analysis_backend_model_version}/{model_version}_{system_role}_{lang}+{translator_provider_forward}{suffix}_tr+{translator_provider_backward}_performance_v2.csv\"\n",
    "pivot_file_tmpl = \"data/language_results/B={analysis_backend_model_version}/{model_version}_{system_role}_LANGs+{translator_provider_forward}{suffix}_tr+{translator_provider_backward}_pivot.csv\"\n",
    "pivot_file_by_country_tmpl = \"data/language_results/B={analysis_backend_model_version}/{model_version}_{system_role}_COUNTRIES+{translator_provider_forward}{suffix}_tr+{translator_provider_backward}_pivot.csv\"\n",
    "\n",
    "LANGUAGES = [\n",
    "    \"af\",\n",
    "    \"am\",\n",
    "    \"ar\",\n",
    "    \"az\",\n",
    "    \"be\",\n",
    "    \"bg\",\n",
    "    \"bn\",\n",
    "    \"bs\",\n",
    "    \"ca\",\n",
    "    \"ceb\",\n",
    "    \"co\",\n",
    "    \"cs\",\n",
    "    \"cy\",\n",
    "    \"da\",\n",
    "    \"de\",\n",
    "    \"el\",\n",
    "    \"en\",\n",
    "    \"eo\",\n",
    "    \"es\",\n",
    "    \"et\",\n",
    "    \"eu\",\n",
    "    \"fa\",\n",
    "    \"fi\",\n",
    "    \"fr\",\n",
    "    \"fy\",\n",
    "    \"ga\",\n",
    "    \"gd\",\n",
    "    \"gl\",\n",
    "    \"gu\",\n",
    "    \"ha\",\n",
    "    \"haw\",\n",
    "    \"he\",\n",
    "    \"hi\",\n",
    "    \"hmn\",\n",
    "    \"hr\",\n",
    "    \"ht\",\n",
    "    \"hu\",\n",
    "    \"hy\",\n",
    "    \"id\",\n",
    "    \"ig\",\n",
    "    \"is\",\n",
    "    \"it\",\n",
    "    \"iw\",\n",
    "    \"ja\",\n",
    "    \"jw\",\n",
    "    \"ka\",\n",
    "    \"kk\",\n",
    "    \"km\",\n",
    "    \"kn\",\n",
    "    \"ko\",\n",
    "    \"ku\",\n",
    "    \"ky\",\n",
    "    \"la\",\n",
    "    \"lb\",\n",
    "    \"lo\",\n",
    "    \"lt\",\n",
    "    \"lv\",\n",
    "    \"mg\",\n",
    "    \"mi\",\n",
    "    \"mk\",\n",
    "    \"ml\",\n",
    "    \"mn\",\n",
    "    \"mr\",\n",
    "    \"ms\",\n",
    "    \"mt\",\n",
    "    \"my\",\n",
    "    \"ne\",\n",
    "    \"nl\",\n",
    "    \"no\",\n",
    "    \"ny\",\n",
    "    \"or\",\n",
    "    \"pa\",\n",
    "    \"pl\",\n",
    "    \"ps\",\n",
    "    \"pt\",\n",
    "    \"ro\",\n",
    "    \"ru\",\n",
    "    \"sd\",\n",
    "    \"si\",\n",
    "    \"sk\",\n",
    "    \"sl\",\n",
    "    \"sm\",\n",
    "    \"sn\",\n",
    "    \"so\",\n",
    "    \"sq\",\n",
    "    \"sr\",\n",
    "    \"st\",\n",
    "    \"su\",\n",
    "    \"sv\",\n",
    "    \"sw\",\n",
    "    \"ta\",\n",
    "    \"te\",\n",
    "    \"tg\",\n",
    "    \"th\",\n",
    "    \"tl\",\n",
    "    \"tr\",\n",
    "    \"ug\",\n",
    "    \"uk\",\n",
    "    \"ur\",\n",
    "    \"uz\",\n",
    "    \"vi\",\n",
    "    \"xh\",\n",
    "    \"yi\",\n",
    "    \"yo\",\n",
    "    \"zh-cn\",\n",
    "    \"zh-tw\",\n",
    "    \"zu\",\n",
    "]\n",
    "\n",
    "\n",
    "COUNTRIES = [\n",
    "    \"abw\",\n",
    "    \"afg\",\n",
    "    \"ago\",\n",
    "    \"aia\",\n",
    "    \"ala\",\n",
    "    \"alb\",\n",
    "    \"and\",\n",
    "    \"are\",\n",
    "    \"arg\",\n",
    "    \"arm\",\n",
    "    \"asm\",\n",
    "    \"ata\",\n",
    "    \"atf\",\n",
    "    \"atg\",\n",
    "    \"aus\",\n",
    "    \"aut\",\n",
    "    \"aze\",\n",
    "    \"bdi\",\n",
    "    \"bel\",\n",
    "    \"ben\",\n",
    "    \"bes\",\n",
    "    \"bfa\",\n",
    "    \"bgd\",\n",
    "    \"bgr\",\n",
    "    \"bhr\",\n",
    "    \"bhs\",\n",
    "    \"bih\",\n",
    "    \"blm\",\n",
    "    \"blr\",\n",
    "    \"blz\",\n",
    "    \"bmu\",\n",
    "    \"bol\",\n",
    "    \"bra\",\n",
    "    \"brb\",\n",
    "    \"brn\",\n",
    "    \"btn\",\n",
    "    \"bvt\",\n",
    "    \"bwa\",\n",
    "    \"caf\",\n",
    "    \"can\",\n",
    "    \"cck\",\n",
    "    \"che\",\n",
    "    \"chl\",\n",
    "    \"chn\",\n",
    "    \"civ\",\n",
    "    \"cmr\",\n",
    "    \"cod\",\n",
    "    \"cog\",\n",
    "    \"cok\",\n",
    "    \"col\",\n",
    "    \"com\",\n",
    "    \"cpv\",\n",
    "    \"cri\",\n",
    "    \"cub\",\n",
    "    \"cuw\",\n",
    "    \"cxr\",\n",
    "    \"cym\",\n",
    "    \"cyp\",\n",
    "    \"cze\",\n",
    "    \"deu\",\n",
    "    \"dji\",\n",
    "    \"dma\",\n",
    "    \"dnk\",\n",
    "    \"dom\",\n",
    "    \"dza\",\n",
    "    \"ecu\",\n",
    "    \"egy\",\n",
    "    \"eri\",\n",
    "    \"esh\",\n",
    "    \"esp\",\n",
    "    \"est\",\n",
    "    \"eth\",\n",
    "    \"fin\",\n",
    "    \"fji\",\n",
    "    \"flk\",\n",
    "    \"fra\",\n",
    "    \"fro\",\n",
    "    \"fsm\",\n",
    "    \"gab\",\n",
    "    \"gbr\",\n",
    "    \"geo\",\n",
    "    \"ggy\",\n",
    "    \"gha\",\n",
    "    \"gib\",\n",
    "    \"gin\",\n",
    "    \"glp\",\n",
    "    \"gmb\",\n",
    "    \"gnb\",\n",
    "    \"gnq\",\n",
    "    \"grc\",\n",
    "    \"grd\",\n",
    "    \"grl\",\n",
    "    \"gtm\",\n",
    "    \"guf\",\n",
    "    \"gum\",\n",
    "    \"guy\",\n",
    "    \"hkg\",\n",
    "    \"hmd\",\n",
    "    \"hnd\",\n",
    "    \"hrv\",\n",
    "    \"hti\",\n",
    "    \"hun\",\n",
    "    \"idn\",\n",
    "    \"imn\",\n",
    "    \"ind\",\n",
    "    \"iot\",\n",
    "    \"irl\",\n",
    "    \"irn\",\n",
    "    \"irq\",\n",
    "    \"isl\",\n",
    "    \"isr\",\n",
    "    \"ita\",\n",
    "    \"jam\",\n",
    "    \"jey\",\n",
    "    \"jor\",\n",
    "    \"jpn\",\n",
    "    \"kaz\",\n",
    "    \"ken\",\n",
    "    \"kgz\",\n",
    "    \"khm\",\n",
    "    \"kir\",\n",
    "    \"kna\",\n",
    "    \"kor\",\n",
    "    \"kwt\",\n",
    "    \"lao\",\n",
    "    \"lbn\",\n",
    "    \"lbr\",\n",
    "    \"lby\",\n",
    "    \"lca\",\n",
    "    \"lie\",\n",
    "    \"lka\",\n",
    "    \"lso\",\n",
    "    \"ltu\",\n",
    "    \"lux\",\n",
    "    \"lva\",\n",
    "    \"mac\",\n",
    "    \"maf\",\n",
    "    \"mar\",\n",
    "    \"mco\",\n",
    "    \"mda\",\n",
    "    \"mdg\",\n",
    "    \"mdv\",\n",
    "    \"mex\",\n",
    "    \"mhl\",\n",
    "    \"mkd\",\n",
    "    \"mli\",\n",
    "    \"mlt\",\n",
    "    \"mmr\",\n",
    "    \"mne\",\n",
    "    \"mng\",\n",
    "    \"mnp\",\n",
    "    \"moz\",\n",
    "    \"mrt\",\n",
    "    \"msr\",\n",
    "    \"mtq\",\n",
    "    \"mus\",\n",
    "    \"mwi\",\n",
    "    \"mys\",\n",
    "    \"myt\",\n",
    "    \"nam\",\n",
    "    \"ncl\",\n",
    "    \"ner\",\n",
    "    \"nfk\",\n",
    "    \"nga\",\n",
    "    \"nic\",\n",
    "    \"niu\",\n",
    "    \"nld\",\n",
    "    \"nor\",\n",
    "    \"npl\",\n",
    "    \"nru\",\n",
    "    \"nzl\",\n",
    "    \"omn\",\n",
    "    \"pak\",\n",
    "    \"pan\",\n",
    "    \"pcn\",\n",
    "    \"per\",\n",
    "    \"phl\",\n",
    "    \"plw\",\n",
    "    \"png\",\n",
    "    \"pol\",\n",
    "    \"pri\",\n",
    "    \"prk\",\n",
    "    \"prt\",\n",
    "    \"pry\",\n",
    "    \"pse\",\n",
    "    \"pyf\",\n",
    "    \"qat\",\n",
    "    \"reu\",\n",
    "    \"rou\",\n",
    "    \"rus\",\n",
    "    \"rwa\",\n",
    "    \"sau\",\n",
    "    \"sdn\",\n",
    "    \"sen\",\n",
    "    \"sgp\",\n",
    "    \"sgs\",\n",
    "    \"shn\",\n",
    "    \"sjm\",\n",
    "    \"slb\",\n",
    "    \"sle\",\n",
    "    \"slv\",\n",
    "    \"smr\",\n",
    "    \"som\",\n",
    "    \"spm\",\n",
    "    \"srb\",\n",
    "    \"ssd\",\n",
    "    \"stp\",\n",
    "    \"sur\",\n",
    "    \"svk\",\n",
    "    \"svn\",\n",
    "    \"swe\",\n",
    "    \"swz\",\n",
    "    \"sxm\",\n",
    "    \"syc\",\n",
    "    \"syr\",\n",
    "    \"tca\",\n",
    "    \"tcd\",\n",
    "    \"tgo\",\n",
    "    \"tha\",\n",
    "    \"tjk\",\n",
    "    \"tkl\",\n",
    "    \"tkm\",\n",
    "    \"tls\",\n",
    "    \"ton\",\n",
    "    \"tto\",\n",
    "    \"tun\",\n",
    "    \"tur\",\n",
    "    \"tuv\",\n",
    "    \"twn\",\n",
    "    \"tza\",\n",
    "    \"uga\",\n",
    "    \"ukr\",\n",
    "    \"umi\",\n",
    "    \"ury\",\n",
    "    \"usa\",\n",
    "    \"uzb\",\n",
    "    \"vat\",\n",
    "    \"vct\",\n",
    "    \"ven\",\n",
    "    \"vgb\",\n",
    "    \"vir\",\n",
    "    \"vnm\",\n",
    "    \"vut\",\n",
    "    \"wlf\",\n",
    "    \"wsm\",\n",
    "    \"yem\",\n",
    "    \"zaf\",\n",
    "    \"zmb\",\n",
    "    \"zwe\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(in_path):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m data_source \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_response_en\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "system_role = \"normal\"\n",
    "translator_provider_forward = \"google\"\n",
    "translator_provider_backward = \"google\"\n",
    "analysis_backend_model_version = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "add_paraphrase = False\n",
    "country = None\n",
    "\n",
    "for model_version in MODELS:\n",
    "    for lang in LANGUAGES:\n",
    "        in_path = cache_parse_responses_tmpl.format(\n",
    "                    model_version=get_model_name_path(model_version),\n",
    "                    system_role=system_role,\n",
    "                    lang=lang,\n",
    "                    suffix=get_suffix(add_paraphrase, country),\n",
    "                    translator_provider_forward=translator_provider_forward,\n",
    "                    translator_provider_backward=translator_provider_backward,\n",
    "                    analysis_backend_model_version=get_model_name_path(\n",
    "                        analysis_backend_model_version\n",
    "                    ),\n",
    "                )\n",
    "        import pandas as pd\n",
    "        BASE_DIR =\"../../\"\n",
    "        in_path = os.path.join(BASE_DIR, in_path)\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "        df = pd.read_csv(in_path)\n",
    "\n",
    "        data_source = df[\"gpt_response_en\"]\n",
    "\n",
    "        query=f\"Does the following answer make sense? {data_source}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrolleyClean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
