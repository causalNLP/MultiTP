{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(in_path):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m data_source \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_response_en\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/TrolleyClean/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from multi_tp.models_ids import *\n",
    "from multi_tp.utils import get_model_name_path, get_suffix, LANGUAGES, performance_file_v2_tmpl, pivot_file_tmpl, cache_parse_responses_tmpl, cache_responses_tmpl\n",
    "import os\n",
    "system_role = \"normal\"\n",
    "translator_provider_forward = \"google\"\n",
    "translator_provider_backward = \"google\"\n",
    "analysis_backend_model_version = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "add_paraphrase = False\n",
    "country = None\n",
    "\n",
    "for model_version in MODELS:\n",
    "    for lang in LANGUAGES:\n",
    "        in_path = cache_parse_responses_tmpl.format(\n",
    "                    model_version=get_model_name_path(model_version),\n",
    "                    system_role=system_role,\n",
    "                    lang=lang,\n",
    "                    suffix=get_suffix(add_paraphrase, country),\n",
    "                    translator_provider_forward=translator_provider_forward,\n",
    "                    translator_provider_backward=translator_provider_backward,\n",
    "                    analysis_backend_model_version=get_model_name_path(\n",
    "                        analysis_backend_model_version\n",
    "                    ),\n",
    "                )\n",
    "        import pandas as pd\n",
    "        BASE_DIR =\"../../\"\n",
    "        in_path = os.path.join(BASE_DIR, in_path)\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "        df = pd.read_csv(in_path)\n",
    "\n",
    "        data_source = df[\"gpt_response_en\"]\n",
    "\n",
    "        query=f\"Does the following answer make sense? {data_source}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/cache_parsing/B={analysis_backend_model_version}/{model_version}_{system_role}_{lang}+{translator_provider_forward}{suffix}_tr+{translator_provider_backward}_response.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_parse_responses_tmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_tp.models_ids import *\n",
    "from multi_tp.utils import get_model_name_path, get_suffix, LANGUAGES, performance_file_v2_tmpl, pivot_file_tmpl, cache_parse_responses_tmpl, cache_responses_tmpl\n",
    "import os\n",
    "system_role = \"normal\"\n",
    "translator_provider_forward = \"google\"\n",
    "translator_provider_backward = \"google\"\n",
    "analysis_backend_model_version = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "add_paraphrase = False\n",
    "country = None\n",
    "\n",
    "\n",
    "model_version=GPT_4_OMNI_MINI\n",
    "\n",
    "acc = []\n",
    "for model_version in MODELS:\n",
    "    for lang in LANGUAGES:\n",
    "        in_path = cache_parse_responses_tmpl.format(\n",
    "                    model_version=get_model_name_path(model_version),\n",
    "                    system_role=system_role,\n",
    "                    lang=lang,\n",
    "                    suffix=get_suffix(add_paraphrase, country),\n",
    "                    translator_provider_forward=translator_provider_forward,\n",
    "                    translator_provider_backward=translator_provider_backward,\n",
    "                    analysis_backend_model_version=get_model_name_path(\n",
    "                        analysis_backend_model_version\n",
    "                    ),\n",
    "                )\n",
    "        import pandas as pd\n",
    "        BASE_DIR =\"../../\"\n",
    "        in_path = os.path.join(BASE_DIR, in_path)\n",
    "        if not os.path.exists(in_path):\n",
    "            continue\n",
    "        df = pd.read_csv(in_path)\n",
    "        # Apply the analysis to your DataFrame\n",
    "        def get_category(row):\n",
    "            prob = row['this_saving_prob']\n",
    "            if prob == 0.5:  # either case\n",
    "                return 'either'\n",
    "            elif prob == -1:  # neither case\n",
    "                return 'neither'\n",
    "            elif prob == -10:  # underskilled case\n",
    "                return 'underskilled'\n",
    "            return 'valid'  # for any other cases\n",
    "        \n",
    "        # Add category column\n",
    "        df['category'] = df.apply(get_category, axis=1)\n",
    "        # result_df['model_version'] = model_version\n",
    "        # result_df['lang'] = lang\n",
    "        tmp = df[\"category\"].value_counts() / len(df) * 100\n",
    "        tmp = pd.DataFrame(tmp).T\n",
    "        tmp['model_version'] = model_version\n",
    "        tmp['lang'] = lang\n",
    "        tmp[\"legible\"] = 100 - tmp[\"underskilled\"] if \"underskilled\" in tmp else 100\n",
    "        acc.append(tmp)\n",
    "\n",
    "result_df = pd.concat(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model_version                               |   Perc. language (valid >= 80)  |   Perc. language (80 > valid >= 70) |   Perc. language (valid < 70) |\n",
      "|:--------------------------------------------|--------------------------------:|------------------------------------:|------------------------------:|\n",
      "| mistralai_Mistral-7B-Instruct-v0.2          |                       11.215    |                             8.41121 |                       80.3738 |\n",
      "| meta-llama_Meta-Llama-3.1-8B-Instruct       |                        5.60748  |                            14.9533  |                       79.4393 |\n",
      "| neuralmagic_Meta-Llama-3.1-70B-Instruct-FP8 |                       28.3019   |                            28.3019  |                       43.3962 |\n",
      "| meta-llama_Meta-Llama-3-8B-Instruct         |                       39.2157   |                            20.5882  |                       40.1961 |\n",
      "| neuralmagic_Meta-Llama-3-70B-Instruct-FP8   |                       50.4673   |                            19.6262  |                       29.9065 |\n",
      "| meta-llama_Llama-2-70b-chat-hf              |                       10.4762   |                             5.71429 |                       83.8095 |\n",
      "| meta-llama_Llama-2-13b-chat-hf              |                        4.6729   |                             1.86916 |                       93.4579 |\n",
      "| meta-llama_Llama-2-7b-chat-hf               |                       19.6262   |                             7.47664 |                       72.8972 |\n",
      "| Qwen_Qwen2-7B-Instruct                      |                        5.60748  |                             7.47664 |                       86.9159 |\n",
      "| Qwen_Qwen2-72B-Instruct-GPTQ-Int8           |                        0.970874 |                             2.91262 |                       96.1165 |\n",
      "| microsoft_Phi-3-medium-4k-instruct          |                        0        |                             1.86916 |                       98.1308 |\n",
      "| microsoft_Phi-3.5-mini-instruct             |                        9.47368  |                            14.7368  |                       75.7895 |\n",
      "| microsoft_Phi-3.5-MoE-instruct              |                        0        |                             0       |                      100      |\n",
      "| google_gemma-2-9b-it                        |                        0        |                             0       |                      100      |\n",
      "| google_gemma-2b-it                          |                        4.7619   |                             7.61905 |                       87.619  |\n",
      "| google_gemma-2-27b-it                       |                        0        |                             4.30108 |                       95.6989 |\n",
      "| gpt-4o-mini-2024-07-18                      |                        0        |                             0       |                      100      |\n"
     ]
    }
   ],
   "source": [
    "#  LLAMA_2_70B, LLAMA_3_1_70B\n",
    "acc = []\n",
    "key = \"valid\"\n",
    "for model in MODELS:\n",
    "    a = result_df[result_df[\"model_version\"].isin([model])]\n",
    "    if len(a) == 0:\n",
    "        continue\n",
    "    acc.append(pd.DataFrame({\n",
    "        \"model_version\": get_model_name_path(model),\n",
    "        \"Perc. language (valid >= 80) \":len(a[a[key] >= 80]) / len(a) * 100,\n",
    "        \"Perc. language (80 > valid >= 70)\":len(a[a[key].apply(lambda x : x >= 70 and x < 80)]) / len(a) * 100,\n",
    "        \"Perc. language (valid < 70)\":len(a[a[key].apply(lambda x : x < 70)]) / len(a) * 100,\n",
    "    }, index=[0]))\n",
    "print(pd.concat(acc).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model_version                               |   skilled_95 |   skilled_90 |   skilled_below_90 |\n",
      "|:--------------------------------------------|-------------:|-------------:|-------------------:|\n",
      "| mistralai_Mistral-7B-Instruct-v0.2          |     100      |     0        |                  0 |\n",
      "| meta-llama_Meta-Llama-3.1-8B-Instruct       |     100      |     0        |                  0 |\n",
      "| neuralmagic_Meta-Llama-3.1-70B-Instruct-FP8 |     100      |     0        |                  0 |\n",
      "| meta-llama_Meta-Llama-3-8B-Instruct         |     100      |     0        |                  0 |\n",
      "| neuralmagic_Meta-Llama-3-70B-Instruct-FP8   |     100      |     0        |                  0 |\n",
      "| meta-llama_Llama-2-70b-chat-hf              |     100      |     0        |                  0 |\n",
      "| meta-llama_Llama-2-13b-chat-hf              |      98.1308 |     1.86916  |                  0 |\n",
      "| meta-llama_Llama-2-7b-chat-hf               |      99.0654 |     0.934579 |                  0 |\n",
      "| Qwen_Qwen2-7B-Instruct                      |     100      |     0        |                  0 |\n",
      "| Qwen_Qwen2-72B-Instruct-GPTQ-Int8           |     100      |     0        |                  0 |\n",
      "| microsoft_Phi-3-medium-4k-instruct          |     100      |     0        |                  0 |\n",
      "| microsoft_Phi-3.5-mini-instruct             |     100      |     0        |                  0 |\n",
      "| microsoft_Phi-3.5-MoE-instruct              |     100      |     0        |                  0 |\n",
      "| google_gemma-2-9b-it                        |     100      |     0        |                  0 |\n",
      "| google_gemma-2b-it                          |     100      |     0        |                  0 |\n",
      "| google_gemma-2-27b-it                       |     100      |     0        |                  0 |\n",
      "| gpt-4o-mini-2024-07-18                      |     100      |     0        |                  0 |\n"
     ]
    }
   ],
   "source": [
    "#  LLAMA_2_70B, LLAMA_3_1_70B\n",
    "acc = []\n",
    "key = \"legible\"\n",
    "for model in MODELS:\n",
    "    a = result_df[result_df[\"model_version\"].isin([model])]\n",
    "    if len(a) == 0:\n",
    "        continue\n",
    "    acc.append(pd.DataFrame({\n",
    "        \"model_version\": get_model_name_path(model),\n",
    "        \"skilled_95\":len(a[a[key] >= 95]) / len(a) * 100,\n",
    "        \"skilled_90\":len(a[a[key].apply(lambda x : x > 90 and x <= 95)]) / len(a) * 100,\n",
    "        \"skilled_below_90\":len(a[a[key].apply(lambda x : x < 90)]) / len(a) * 100,\n",
    "    }, index=[0]))\n",
    "print(pd.concat(acc).to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrolleyClean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
